{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"<span style=\"font-size:18px;\"> The context of this notebook is Umojahack's advanced challenge 2022, organized by Instadeep and hosted by Zindi.<br>\nThe provided data presents amino acid sequences extracted from different species of snakes as well as different kinds antivenoms tested on it, meanwhile the target is a signal that expresses the binding of the antivenom with K-mers (of length 16) coming from the toxin's sequence.","metadata":{}},{"cell_type":"markdown","source":"# Notebook Setup","metadata":{}},{"cell_type":"code","source":"%load_ext tensorboard","metadata":{"execution":{"iopub.status.busy":"2022-03-23T14:07:24.360130Z","iopub.execute_input":"2022-03-23T14:07:24.360467Z","iopub.status.idle":"2022-03-23T14:07:24.403222Z","shell.execute_reply.started":"2022-03-23T14:07:24.360389Z","shell.execute_reply":"2022-03-23T14:07:24.402469Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import random, gc, time, os\nimport warnings\nwarnings.simplefilter('ignore')\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nfrom sklearn.model_selection import GroupKFold \nfrom torch import nn \nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nfrom transformers import BertModel, BertTokenizer\nfrom torch.utils.tensorboard import SummaryWriter","metadata":{"execution":{"iopub.status.busy":"2022-03-24T08:54:18.791025Z","iopub.execute_input":"2022-03-24T08:54:18.791291Z","iopub.status.idle":"2022-03-24T08:54:21.862613Z","shell.execute_reply.started":"2022-03-24T08:54:18.791259Z","shell.execute_reply":"2022-03-24T08:54:21.861666Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"def make_reproduceable(SEED=8):\n    random.seed(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    torch.backends.cudnn.deterministic = True","metadata":{"execution":{"iopub.status.busy":"2022-03-24T08:54:21.864505Z","iopub.execute_input":"2022-03-24T08:54:21.864772Z","iopub.status.idle":"2022-03-24T08:54:21.870536Z","shell.execute_reply.started":"2022-03-24T08:54:21.864735Z","shell.execute_reply":"2022-03-24T08:54:21.868853Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Thanks to Assazzin (https://github.com/ASSAZZIN-01) for this wonderful function\ndef free_memory(sleep_time=0.1):\n    \"\"\" Black magic function to free torch memory and some jupyter whims \"\"\"\n    gc.collect()\n    torch.cuda.synchronize()\n    gc.collect()\n    torch.cuda.empty_cache()\n    time.sleep(sleep_time)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T08:54:21.872155Z","iopub.execute_input":"2022-03-24T08:54:21.872578Z","iopub.status.idle":"2022-03-24T08:54:21.880746Z","shell.execute_reply.started":"2022-03-24T08:54:21.872538Z","shell.execute_reply":"2022-03-24T08:54:21.880078Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Data Preparation","metadata":{}},{"cell_type":"code","source":"class CSV_Data:\n    def __init__(self, train_path, test_path):\n        self.train = pd.read_csv(train_path)\n        self.test = pd.read_csv(test_path)\n        self.bert_map = self.seq_mapper(using = \"Rostlab/prot_bert\")\n        free_memory()\n        \n    def seq_mapper(self, column = \"Toxin_Kmer\", using=None):\n        s = set()\n        for _, x in self.train.iterrows():\n            l = []\n            s = s.union(set(x[column]))\n        for _, x in self.test.iterrows():\n            l = []\n            s = s.union(set(x[column]))\n        vocab_size = len(s)\n        chrs = list(s)\n        if (using == None):\n            ix = list(range(1,vocab_size+1))\n            voc_map = {i:j for i,j in zip(chrs,ix)}\n        else :\n            tokenizer = BertTokenizer.from_pretrained(using, do_lower_case=False ) #\"Rostlab/prot_bert\"\n            model = BertModel.from_pretrained(using) #\"Rostlab/prot_bert\"\n            voc_map = {i:(model(**tokenizer(i, return_tensors = \"pt\"))[0][0,2,:]).detach().numpy() for i in chrs}\n        return voc_map\n            \n    def species_mapper(self):\n        l = []\n        for i in list(set(self.train.Species).union(set(self.test.Species))):\n            l.extend(i.split('_'))\n        species_voc_set = list(set(l))\n        species_voc_set_map = { k:v for k , v in zip(species_voc_set,range(1,len(species_voc_set)+1))}\n        return species_voc_set_map\n    \n    def others_mapper(self, column):\n        voc_map = list(set(self.train[column]).union(set(self.test[column])))\n        vocab_size = len(voc_map)\n        ix = range(1, len(voc_map)+1)\n        voc_map = {i:j for i,j in zip(voc_map, ix)}\n        return voc_map\n        \n    def add_bert_emb(self):\n        if (\"kmer_bert\" in data.train.columns):\n            pass\n        else:\n            print(\"- Adding Bert embedding feature...\")\n            tic = time.time()\n            self.train[\"kmer_bert\"] = self.train.Toxin_Kmer.apply(lambda x: [self.bert_map[e] for e in x])\n            self.test[\"kmer_bert\"] = self.test.Toxin_Kmer.apply(lambda x: [self.bert_map[e] for e in x])\n            toc = time.time()\n            print(f\"- Adding Bert embedding feature finished in {toc-tic} seconds.\")\n            \n    \n    def concatenate(self):\n        if (\"sequence\" in data.train.columns):\n            pass\n        else:\n            print(\"- Concatenation operation starting...\")\n            tic = time.time()\n            # For the train set\n            cur_pos = self.train.iloc[0][\"Kmer_Position_start\"]\n            cur_tox = self.train.iloc[0][\"Toxin_UniprotID\"]\n            seq = self.train.iloc[0][\"Toxin_Kmer\"]\n            lengths = []\n            length = 1\n            seq_list = []\n            for i, x in self.train.iterrows():\n                if (i==0):\n                    continue\n                elif (i!=len(self.train)-1):\n                    if ((x[\"Kmer_Position_start\"]>cur_pos) and (x[\"Toxin_UniprotID\"]==cur_tox)):\n                        seq += x[\"Toxin_Kmer\"][-1]\n                        length += 1\n                    else:\n                        seq_list.append(seq)\n                        lengths.append(length)\n                        seq = x[\"Toxin_Kmer\"]\n                        length = 1\n                        cur_tox = x[\"Toxin_UniprotID\"]\n                    cur_pos = x[\"Kmer_Position_start\"]\n\n                else:\n                    if ((x[\"Kmer_Position_start\"]>cur_pos) and (x[\"Toxin_UniprotID\"]==cur_tox)):\n                        seq += x[\"Toxin_Kmer\"][-1]\n                        length += 1\n                        seq_list.append(seq)\n                        lengths.append(length)\n                    else:\n                        seq_list.append(seq)\n                        lengths.append(length)\n                        seq_list.append(x[\"Toxin_Kmer\"])\n                        lengths.append(1)\n\n            seq_to_df = [s for i, s in enumerate(seq_list) for _ in range(lengths[i])]\n            self.train[\"sequence\"] = seq_to_df\n\n            # For the test set\n            # For the train set\n            cur_pos = self.test.iloc[0][\"Kmer_Position_start\"]\n            cur_tox = self.test.iloc[0][\"Toxin_UniprotID\"]\n            seq = self.test.iloc[0][\"Toxin_Kmer\"]\n            lengths = []\n            length = 1\n            seq_list = []\n            for i, x in self.test.iterrows():\n                if (i==0):\n                    continue\n                elif (i!=len(self.test)-1):\n                    if ((x[\"Kmer_Position_start\"]>cur_pos) and (x[\"Toxin_UniprotID\"]==cur_tox)):\n                        seq += x[\"Toxin_Kmer\"][-1]\n                        length += 1\n                    else:\n                        seq_list.append(seq)\n                        lengths.append(length)\n                        seq = x[\"Toxin_Kmer\"]\n                        length = 1\n                        cur_tox = x[\"Toxin_UniprotID\"]\n                    cur_pos = x[\"Kmer_Position_start\"]\n\n                else:\n                    if ((x[\"Kmer_Position_start\"]>cur_pos) and (x[\"Toxin_UniprotID\"]==cur_tox)):\n                        seq += x[\"Toxin_Kmer\"][-1]\n                        length += 1\n                        seq_list.append(seq)\n                        lengths.append(length)\n                    else:\n                        seq_list.append(seq)\n                        lengths.append(length)\n                        seq_list.append(x[\"Toxin_Kmer\"])\n                        lengths.append(1)\n\n            seq_to_df = [s for i, s in enumerate(seq_list) for _ in range(lengths[i])]\n            self.test[\"sequence\"] = seq_to_df\n            toc = time.time()\n            print(f\"- Concatenation operation finished in {toc-tic} seconds\")\n\n    def seq_mask(self, max_len=364):\n        if (\"sequence_mask\" in data.train.columns):\n            pass\n        else:\n            self.train[\"sequence_mask\"] = self.train.sequence.apply(lambda x: [0]*len(x)+max(0,max_len-len(x))*[1])\n            self.test[\"sequence_mask\"] = self.test.sequence.apply(lambda x: [0]*len(x)+max(0,max_len-len(x))*[1])\n        ","metadata":{"execution":{"iopub.status.busy":"2022-03-24T08:54:24.106623Z","iopub.execute_input":"2022-03-24T08:54:24.107039Z","iopub.status.idle":"2022-03-24T08:54:24.140021Z","shell.execute_reply.started":"2022-03-24T08:54:24.107003Z","shell.execute_reply":"2022-03-24T08:54:24.139330Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_path = \"https://storage.googleapis.com/umojahack2022/train.csv\"\ntest_path = \"https://storage.googleapis.com/umojahack2022/test.csv\"\ndata = CSV_Data(train_path, test_path)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T08:54:26.520080Z","iopub.execute_input":"2022-03-24T08:54:26.520762Z","iopub.status.idle":"2022-03-24T08:55:25.829399Z","shell.execute_reply.started":"2022-03-24T08:54:26.520726Z","shell.execute_reply":"2022-03-24T08:55:25.828625Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"data.train","metadata":{"execution":{"iopub.status.busy":"2022-03-24T08:55:25.831125Z","iopub.execute_input":"2022-03-24T08:55:25.831390Z","iopub.status.idle":"2022-03-24T08:55:25.860159Z","shell.execute_reply.started":"2022-03-24T08:55:25.831352Z","shell.execute_reply":"2022-03-24T08:55:25.859510Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"data.test","metadata":{"execution":{"iopub.status.busy":"2022-03-24T08:55:25.861431Z","iopub.execute_input":"2022-03-24T08:55:25.861703Z","iopub.status.idle":"2022-03-24T08:55:25.880547Z","shell.execute_reply.started":"2022-03-24T08:55:25.861668Z","shell.execute_reply":"2022-03-24T08:55:25.879763Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"data.train.iloc[5]","metadata":{"execution":{"iopub.status.busy":"2022-03-24T08:55:25.883132Z","iopub.execute_input":"2022-03-24T08:55:25.883400Z","iopub.status.idle":"2022-03-24T08:55:25.891168Z","shell.execute_reply.started":"2022-03-24T08:55:25.883364Z","shell.execute_reply":"2022-03-24T08:55:25.890170Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class Dataset:\n    def __init__(self, data, features, state, selected_train=None, selected_val=None):\n        self.d = data\n        self.features = features\n        if (state == \"Train select\"):\n            self.data = self.d.train.iloc[selected_train]\n        elif (state == \"Val select\"):\n            self.data = self.d.train.iloc[selected_val]\n        elif (state == \"Train\"):\n            self.data = self.d.train\n        else:\n            self.data = self.d.test\n        self.state = state\n        self.d.concatenate()\n        self.d.seq_mask()\n        self.d.add_bert_emb()\n        self.kmer_map = self.d.seq_mapper()\n        self.spec_map = self.d.species_mapper()\n        self.antivenom_map = self.d.others_mapper(\"Antivenom\")\n        self.protfam_map = self.d.others_mapper(\"ProteinFam\")\n\n\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        d = dict()\n        row = self.data.iloc[idx]\n        d[\"kmer\"] = torch.as_tensor([self.kmer_map[e] for e in row[\"Toxin_Kmer\"]])\n        if (\"kmer_bert\" in self.features):\n            d[\"kmer_bert\"] = torch.tensor(row[\"kmer_bert\"], dtype = torch.float)\n        if (\"seq\" in self.features):\n            d[\"seq\"] = torch.tensor([self.kmer_map[e] for e in row[\"sequence\"]]+[0]*(364-len(row[\"sequence\"])), dtype = torch.long)\n        if (\"seq_mask\" in self.features):\n            d[\"seq_mask\"] = torch.tensor(row[\"sequence_mask\"], dtype = torch.uint8)\n        if (\"species\" in self.features):\n            d[\"species\"] = torch.tensor([self.spec_map[e] for e in row[\"Species\"].split(\"_\")], dtype = torch.long)\n        if (\"antivenom\" in self.features):\n            d[\"antivenom\"] = torch.as_tensor(self.antivenom_map[row[\"Antivenom\"]])\n        if (\"protfam\" in self.features):\n            d[\"protfam\"] = torch.tensor(self.protfam_map[row[\"ProteinFam\"]], dtype = torch.long)\n        if (\"pos_start\" in self.features):\n            d[\"pos_start\"] = torch.as_tensor(row[\"Kmer_Position_start\"])\n        \n        if (self.state in [\"Train\", \"Train select\", \"Val select\"]):\n            signal = torch.as_tensor([row[\"Signal\"]])\n            return d, signal\n        return d","metadata":{"execution":{"iopub.status.busy":"2022-03-24T08:55:25.893078Z","iopub.execute_input":"2022-03-24T08:55:25.893342Z","iopub.status.idle":"2022-03-24T08:55:25.910210Z","shell.execute_reply.started":"2022-03-24T08:55:25.893308Z","shell.execute_reply":"2022-03-24T08:55:25.909535Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"make_reproduceable()","metadata":{"execution":{"iopub.status.busy":"2022-03-24T08:55:25.911788Z","iopub.execute_input":"2022-03-24T08:55:25.912132Z","iopub.status.idle":"2022-03-24T08:55:25.920360Z","shell.execute_reply.started":"2022-03-24T08:55:25.912093Z","shell.execute_reply":"2022-03-24T08:55:25.919676Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"train_data = Dataset(data, features = [\"kmer\", \"kmer_bert\", \"antivenom\", \"species\", \"protfam\", \"seq\", \"seq_mask\", \"pos_start\"], state = \"Train\")\ntrain_data_loader = DataLoader(train_data, shuffle=True, batch_size=64, num_workers=2)\nx, y = iter(train_data_loader).next()\n\nprint(f\"K_mer shape: {x['kmer'].shape}\")\nprint(f\"K_mer bert shape: {x['kmer_bert'].shape}\")\nprint(f\"antivenom shape: {x['antivenom'].shape}\")\nprint(f\"species shape: {x['species'].shape}\")\nprint(f\"protfam shape: {x['protfam'].shape}\")\nprint(f\"seq shape: {x['seq'].shape}\")\nprint(f\"seq mask shape: {x['seq_mask'].shape}\")\nprint(f\"pos start shape: {x['pos_start'].shape}\")\nprint(f\"target shape: {y.shape}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-24T08:55:25.921857Z","iopub.execute_input":"2022-03-24T08:55:25.922238Z","iopub.status.idle":"2022-03-24T08:55:45.331503Z","shell.execute_reply.started":"2022-03-24T08:55:25.922199Z","shell.execute_reply":"2022-03-24T08:55:45.330704Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Training and Evaluation Functions","metadata":{}},{"cell_type":"code","source":"def train_func(\n    train_data_loader,\n    val_data_loader,\n    features,\n    model,\n    loss_fn,\n    optimizer,\n    num_epochs,\n    device,\n    writer,\n    early_stopping=5,\n): \n    total_batches = len(train_data_loader)\n    total_batches_val = len(val_data_loader)\n    train_loss = []\n    \n    n_iter = 0\n    for epoch in range(num_epochs): \n        free_memory()\n        tqdm_bar = tqdm(train_data_loader, desc=f\"epoch {epoch}\", position=0) \n        old_val_loss = np.inf\n        waiting = 0\n        model.train()\n        for batch_number, (X, y) in enumerate(tqdm_bar):\n            y = y.type(torch.FloatTensor).to(device)\n            X = {k: X[k].to(device) for k in features}\n            \n            optimizer.zero_grad()\n            pred = model(X)\n            loss = loss_fn(pred, y)\n            loss.backward()\n            \n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n            \n            loss = loss.item()\n            train_loss.append(loss)\n\n            writer.add_scalar(\"loss/train\", loss, n_iter)\n            n_iter += 1\n\n            if batch_number % 25 == 0: \n                tqdm_bar.set_postfix(\n                    {\n                        \"train\": f\"{batch_number}/{total_batches} loss: {loss:.3} epoch loss: {np.mean(train_loss):.3}\",\n                    },\n                )\n\n        val_tqdm_bar = tqdm(\n            val_data_loader, desc=f\"epoch {epoch}\", position=0, leave=True,\n        ) \n        val_loss = []\n        model.eval()\n        with torch.no_grad(): \n            for batch_number, (X, y) in enumerate(val_tqdm_bar):\n                y = y.type(torch.FloatTensor).to(device)\n                X = {k: X[k].to(device) for k in features}\n                \n                pred = model(X)\n                val_loss.append(loss_fn(pred, y).item())\n\n                writer.add_scalar(\"loss/validation\", np.random.random(), n_iter)\n\n                if batch_number % 25 == 0: \n                    val_tqdm_bar.set_postfix(\n                        {\n                            \"valid\": f\"{batch_number}/{total_batches_val} val loss: {np.mean(val_loss):.3}\"\n                        },\n                    )\n        \n        new_val_loss = np.mean(val_loss)\n\n        if new_val_loss > old_val_loss:\n            waiting += 1\n        else:\n            old_val_loss = new_val_loss\n\n        if waiting > early_stopping:\n            print(\"Early Stopping\")\n            break","metadata":{"execution":{"iopub.status.busy":"2022-03-24T08:55:52.574824Z","iopub.execute_input":"2022-03-24T08:55:52.575277Z","iopub.status.idle":"2022-03-24T08:55:52.591337Z","shell.execute_reply.started":"2022-03-24T08:55:52.575236Z","shell.execute_reply":"2022-03-24T08:55:52.590572Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def predict_test(data_loader, paths, device): \n    sub = dict()\n    for i, path in enumerate(paths):\n        model = torch.load(path).to(device)\n        tqdm_bar = tqdm(data_loader, desc=\"Inference\", position=0, leave=True) \n        total_batches = len(tqdm_bar)\n\n        preds = []\n        with torch.no_grad():\n            for batch_number, X in enumerate(tqdm_bar):\n                X= {k: X[k].to(device) for k in features}\n                pred = model(X)\n                preds.append(pred.cpu().numpy())\n\n            preds = np.concatenate(preds)\n        sub[str(i)]=preds\n    return sub","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:41:35.822862Z","iopub.execute_input":"2022-03-24T13:41:35.823116Z","iopub.status.idle":"2022-03-24T13:41:35.829723Z","shell.execute_reply.started":"2022-03-24T13:41:35.823087Z","shell.execute_reply":"2022-03-24T13:41:35.829044Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"def kfold_run(\n    Train,\n    features,\n    Model,\n    params,\n    nsplits,\n    loss_fn,\n    num_epochs,\n    device,\n    batch_size,\n    lr=5e-3,\n    early_stopping=5):\n    \n    groups = Train.train[\"Toxin_UniprotID\"]\n    \n    kf = GroupKFold(n_splits=nsplits)\n    \n    for fold, (tr_ix, val_ix) in enumerate(kf.split(groups, groups=groups)):\n        train_data = Dataset(data, features, selected_train = tr_ix, state = \"Train select\")\n        val_data = Dataset(data, features, selected_val = val_ix, state = \"Val select\")\n        train_data_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, num_workers=0)\n        val_data_loader = DataLoader(val_data, shuffle=False, batch_size=batch_size, num_workers=0)\n        print(f\"-----------------FOLD_{fold+1}-----------------\")\n        model = Model(params).to(device)\n        writer = SummaryWriter()\n        writer.add_graph(model, {k: v.to(device) for k, v in next(iter(train_data_loader))[0].items()})\n        optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n        train_func(train_data_loader,val_data_loader,features,model,loss_fn,optimizer,num_epochs,device,writer, early_stopping)\n        torch.save(model, f\"model_fold{fold+1}.pth\")","metadata":{"execution":{"iopub.status.busy":"2022-03-24T10:20:43.870834Z","iopub.execute_input":"2022-03-24T10:20:43.871816Z","iopub.status.idle":"2022-03-24T10:20:43.885411Z","shell.execute_reply.started":"2022-03-24T10:20:43.871774Z","shell.execute_reply":"2022-03-24T10:20:43.884671Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"class SimpleSeqModel(nn.Module):\n    def __init__(self, params):\n        #K_mer_emb_size=1024,K_mer_nunique=20,antivenom_emb_size=128,antivenom_unique=8,\n        #species_unique=40,species_emb_size=512, protfam_unique=17, protfam_emb_size=256\n \n        super().__init__()\n        self.K_mer_emb_size = params[\"K_mer_emb_size\"]       \n        self.K_mer_nunique = 21             \n        self.antivenom_emb_size = params[\"antivenom_emb_size\"]\n        self.antivenom_unique = 9    \n        \n        self.Kmer_emb_layer = nn.Embedding(\n            num_embeddings=self.K_mer_nunique,\n            embedding_dim=self.K_mer_emb_size,\n        )\n        self.Antivenom_emb = nn.Embedding(\n            num_embeddings=self.antivenom_unique,\n            embedding_dim=self.antivenom_emb_size,\n        )\n    \n        self.protfam_emb = nn.Embedding(\n            num_embeddings=18,\n            embedding_dim=params[\"protfam_emb_size\"],\n        )\n        self.Features = nn.Linear(\n            in_features=self.antivenom_emb_size + params[\"protfam_emb_size\"],\n            out_features=128,\n        )\n        \n        self.Lstm_layer_1 = nn.LSTM(\n            input_size=self.K_mer_emb_size,\n            hidden_size=256,\n            num_layers=1,\n            bidirectional=True,\n            batch_first=True,\n        )\n        self.Lstm_layer_2 = nn.LSTM(\n            input_size=512,\n            hidden_size=256,\n            num_layers=1,\n            bidirectional=False,\n            batch_first=True,\n        )\n        \n        self.Linear_1 = nn.Linear(\n            in_features=self.Lstm_layer_2.hidden_size + self.Features.out_features,\n            out_features=256,\n        )\n        self.relu_1 = nn.ReLU()\n        self.Linear_2 = nn.Linear(\n            in_features=self.Linear_1.out_features, out_features=128,\n        )\n        self.relu_2 = nn.ReLU()\n        self.Output = nn.Linear(\n            in_features=self.Linear_2.out_features, out_features=1,\n        )\n        self.out = nn.Sequential(\n            nn.Linear(in_features=self.Lstm_layer_2.hidden_size + self.Features.out_features,out_features=256),\n            nn.Dropout(0.2),\n            nn.ReLU(inplace = True),\n            nn.Linear(in_features=256,out_features=128),\n            nn.Dropout(0.1),\n            nn.ReLU(inplace = True),\n            nn.Linear(128,1)\n        )\n        \n    def forward(self, inputs):\n        kmer_emb = self.Kmer_emb_layer(inputs[\"kmer\"])\n        antivenom_emb = self.Antivenom_emb(inputs[\"antivenom\"])\n        protfam_emb = self.protfam_emb(inputs[\"protfam\"])\n\n        emb_features = torch.cat((antivenom_emb, protfam_emb), axis=1)\n        features = self.Features(emb_features)\n        \n        lstm_1_seq, (lstm_1_h, lstm1_c) = self.Lstm_layer_1(kmer_emb)\n        lstm_2_seq, (lstm_2_h, lstm2_c) = self.Lstm_layer_2(lstm_1_seq)\n\n        lstm_h = torch.squeeze(lstm_2_h)\n        emb = torch.cat((lstm_h, features), axis=1)\n        \"\"\"linear_1 = self.relu_1(self.Linear_1(emb))\n        linear_2 = self.relu_2(self.Linear_2(linear_1))\"\"\"\n        output = self.out(emb)\n        return output   ","metadata":{"execution":{"iopub.status.busy":"2022-03-24T05:25:10.616540Z","iopub.execute_input":"2022-03-24T05:25:10.617018Z","iopub.status.idle":"2022-03-24T05:25:10.643243Z","shell.execute_reply.started":"2022-03-24T05:25:10.616979Z","shell.execute_reply":"2022-03-24T05:25:10.642456Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"\"\"\"class SimpleSeqModel(nn.Module):\n    def __init__(\n        self,\n        K_mer_emb_size,\n        K_mer_nunique,\n        antivenom_emb_size,\n        antivenom_unique,\n        max_Position_start,\n        Position_start_emb_size,\n    ): \n        super().__init__()\n        self.K_mer_emb_size = K_mer_emb_size       \n        self.K_mer_nunique = K_mer_nunique +1               \n        self.antivenom_emb_size = antivenom_emb_size\n        self.antivenom_unique = antivenom_unique +1   \n        \n        self.Kmer_emb_layer = nn.Embedding(\n            num_embeddings=self.K_mer_nunique,\n            embedding_dim=self.K_mer_emb_size,\n        )\n        self.Antivenom_emb = nn.Embedding(\n            num_embeddings=self.antivenom_unique,\n            embedding_dim=self.antivenom_emb_size,\n        )\n    \n        self.Position_start_emb = nn.Embedding(\n            num_embeddings=max_Position_start,\n            embedding_dim=Position_start_emb_size,\n        )\n        self.Features = nn.Linear(\n            in_features=self.antivenom_emb_size + Position_start_emb_size,\n            out_features=128,\n        )\n        \n        self.Lstm_layer_1 = nn.LSTM(\n            input_size=self.K_mer_emb_size,\n            hidden_size=256,\n            num_layers=1,\n            bidirectional=True,\n            batch_first=True,\n        )\n        self.Lstm_layer_2 = nn.LSTM(\n            input_size=512,\n            hidden_size=256,\n            num_layers=1,\n            bidirectional=False,\n            batch_first=True,\n        )\n        \n        self.Linear_1 = nn.Linear(\n            in_features=self.Lstm_layer_2.hidden_size + self.Features.out_features,\n            out_features=512,\n        )\n        self.relu_1 = nn.ReLU()\n        self.Linear_2 = nn.Linear(\n            in_features=self.Linear_1.out_features, out_features=256,\n        )\n        self.relu_2 = nn.ReLU()\n        self.Output = nn.Linear(\n            in_features=self.Linear_2.out_features, out_features=1,\n        )\n        \n    def forward(self, inputs):\n        kmer_emb = self.Kmer_emb_layer(inputs[\"kmer\"])\n        antivenom_emb = self.Antivenom_emb(inputs[\"antivenom\"])\n        position_start_emb = self.Position_start_emb(inputs[\"pos_start\"])\n\n        emb_features = torch.cat((antivenom_emb, position_start_emb), axis=1)\n        features = self.Features(emb_features)\n        \n        lstm_1_seq, (lstm_1_h, lstm1_c) = self.Lstm_layer_1(kmer_emb)\n        lstm_2_seq, (lstm_2_h, lstm2_c) = self.Lstm_layer_2(lstm_1_seq)\n\n        lstm_h = torch.squeeze(lstm_2_h)\n        emb = torch.cat((lstm_h, features), axis=1)\n        linear_1 = self.relu_1(self.Linear_1(emb))\n        linear_2 = self.relu_2(self.Linear_2(linear_1))\n        output = self.Output(linear_2)\n        return output\n        \n        \"\"\"","metadata":{"execution":{"iopub.status.busy":"2022-03-24T05:21:20.016212Z","iopub.status.idle":"2022-03-24T05:21:20.017117Z","shell.execute_reply.started":"2022-03-24T05:21:20.016776Z","shell.execute_reply":"2022-03-24T05:21:20.016842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\"K_mer_emb_size\":512, \"protfam_emb_size\": 64, \"antivenom_emb_size\":64}\nfeatures = [\"kmer\", \"antivenom\", \"protfam\"]","metadata":{"execution":{"iopub.status.busy":"2022-03-23T16:18:16.079210Z","iopub.execute_input":"2022-03-23T16:18:16.079764Z","iopub.status.idle":"2022-03-23T16:18:16.083755Z","shell.execute_reply.started":"2022-03-23T16:18:16.079725Z","shell.execute_reply":"2022-03-23T16:18:16.083061Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"kfold_run(\n    data,\n    features,\n    SimpleSeqModel,\n    params,\n    nsplits=5,\n    loss_fn=nn.MSELoss(),\n    num_epochs=10,\n    device=torch.device(\"cuda\"),\n    batch_size=512,\n    early_stopping=5)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T16:30:13.623810Z","iopub.execute_input":"2022-03-23T16:30:13.624089Z","iopub.status.idle":"2022-03-23T16:56:54.546720Z","shell.execute_reply.started":"2022-03-23T16:30:13.624059Z","shell.execute_reply":"2022-03-23T16:56:54.545815Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"paths = [\"model_fold\"+str(i)+\".pth\" for i in range(1,6)]","metadata":{"execution":{"iopub.status.busy":"2022-03-23T16:56:59.462067Z","iopub.execute_input":"2022-03-23T16:56:59.462521Z","iopub.status.idle":"2022-03-23T16:56:59.467016Z","shell.execute_reply.started":"2022-03-23T16:56:59.462483Z","shell.execute_reply":"2022-03-23T16:56:59.466297Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"test_data = Dataset(data, features, state = \"Test\")\ntest_data_loader = DataLoader(test_data, shuffle=False, batch_size=512, num_workers=0)\nsimplesub = predict_test(test_data_loader,paths, torch.device(\"cuda\"))","metadata":{"execution":{"iopub.status.busy":"2022-03-23T16:57:28.279102Z","iopub.execute_input":"2022-03-23T16:57:28.279380Z","iopub.status.idle":"2022-03-23T16:58:08.094060Z","shell.execute_reply.started":"2022-03-23T16:57:28.279351Z","shell.execute_reply":"2022-03-23T16:58:08.092824Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame(columns = [\"ID\", \"Signal\"])\nsubmission[\"ID\"] = data.test[\"ID\"]\nsubmission[\"Signal\"] = 0.2*(simplesub['0']+simplesub['1']+simplesub['2']+simplesub['3']+simplesub['4'])\nplt.hist(submission[\"Signal\"])\nsubmission.to_csv(\"firstmodelubmission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2022-03-23T16:58:59.425801Z","iopub.execute_input":"2022-03-23T16:58:59.426412Z","iopub.status.idle":"2022-03-23T16:58:59.758442Z","shell.execute_reply.started":"2022-03-23T16:58:59.426372Z","shell.execute_reply":"2022-03-23T16:58:59.757616Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"class SimpleSeqBertModel(nn.Module):\n    def __init__(self, params):\n        #K_mer_emb_size=1024,K_mer_nunique=20,antivenom_emb_size=128,antivenom_unique=8,\n        #species_unique=40,species_emb_size=512, protfam_unique=17, protfam_emb_size=256\n \n        super().__init__()\n        #self.K_mer_emb_size = params[\"K_mer_emb_size\"]       \n        self.K_mer_nunique = 21             \n        self.antivenom_emb_size = params[\"antivenom_emb_size\"]\n        self.antivenom_unique = 9    \n        \n        \"\"\"self.Kmer_emb_layer = nn.Embedding(\n            num_embeddings=self.K_mer_nunique,\n            embedding_dim=self.K_mer_emb_size,\n        )\"\"\"\n        self.Antivenom_emb = nn.Embedding(\n            num_embeddings=self.antivenom_unique,\n            embedding_dim=self.antivenom_emb_size,\n        )\n    \n        self.protfam_emb = nn.Embedding(\n            num_embeddings=18,\n            embedding_dim=params[\"protfam_emb_size\"],\n        )\n        self.Features = nn.Linear(\n            in_features=self.antivenom_emb_size + params[\"protfam_emb_size\"],\n            out_features=128,\n        )\n        \n        self.Lstm_layer_1 = nn.LSTM(\n            input_size=1024,\n            hidden_size=256,\n            num_layers=1,\n            bidirectional=True,\n            batch_first=True,\n        )\n        self.Lstm_layer_2 = nn.LSTM(\n            input_size=512,\n            hidden_size=256,\n            num_layers=1,\n            bidirectional=False,\n            batch_first=True,\n        )\n        \n        self.Linear_1 = nn.Linear(\n            in_features=self.Lstm_layer_2.hidden_size + self.Features.out_features,\n            out_features=256,\n        )\n        self.relu_1 = nn.ReLU()\n        self.Linear_2 = nn.Linear(\n            in_features=self.Linear_1.out_features, out_features=128,\n        )\n        self.relu_2 = nn.ReLU()\n        self.Output = nn.Linear(\n            in_features=self.Linear_2.out_features, out_features=1,\n        )\n        self.out = nn.Sequential(\n            nn.Linear(in_features=self.Lstm_layer_2.hidden_size + self.Features.out_features,out_features=256),\n            nn.Dropout(0.2),\n            nn.ReLU(inplace = True),\n            nn.Linear(in_features=256,out_features=128),\n            nn.Dropout(0.2),\n            nn.ReLU(inplace = True),\n            nn.Linear(128,1)\n        )\n        \n    def forward(self, inputs):\n        kmer_emb = inputs[\"kmer_bert\"]\n        antivenom_emb = self.Antivenom_emb(inputs[\"antivenom\"])\n        protfam_emb = self.protfam_emb(inputs[\"protfam\"])\n\n        emb_features = torch.cat((antivenom_emb, protfam_emb), axis=1)\n        features = self.Features(emb_features)\n        \n        lstm_1_seq, (lstm_1_h, lstm1_c) = self.Lstm_layer_1(kmer_emb)\n        lstm_2_seq, (lstm_2_h, lstm2_c) = self.Lstm_layer_2(lstm_1_seq)\n\n        lstm_h = torch.squeeze(lstm_2_h)\n        emb = torch.cat((lstm_h, features), axis=1)\n        \"\"\"linear_1 = self.relu_1(self.Linear_1(emb))\n        linear_2 = self.relu_2(self.Linear_2(linear_1))\"\"\"\n        output = self.out(emb)\n        return output   ","metadata":{"execution":{"iopub.status.busy":"2022-03-24T07:08:12.394985Z","iopub.execute_input":"2022-03-24T07:08:12.395241Z","iopub.status.idle":"2022-03-24T07:08:12.412139Z","shell.execute_reply.started":"2022-03-24T07:08:12.395211Z","shell.execute_reply":"2022-03-24T07:08:12.411189Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"params = {\"protfam_emb_size\": 64, \"antivenom_emb_size\":64}\nfeatures = [\"kmer_bert\", \"antivenom\", \"protfam\"]","metadata":{"execution":{"iopub.status.busy":"2022-03-24T07:08:14.891451Z","iopub.execute_input":"2022-03-24T07:08:14.891828Z","iopub.status.idle":"2022-03-24T07:08:14.895896Z","shell.execute_reply.started":"2022-03-24T07:08:14.891794Z","shell.execute_reply":"2022-03-24T07:08:14.895045Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"kfold_run(\n    data,\n    features,\n    SimpleSeqBertModel,\n    params,\n    nsplits=5,\n    loss_fn=nn.MSELoss(),\n    num_epochs=10,\n    device=torch.device(\"cuda\"),\n    batch_size=512,\n    early_stopping=5)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T07:08:15.466585Z","iopub.execute_input":"2022-03-24T07:08:15.467043Z","iopub.status.idle":"2022-03-24T07:09:10.763585Z","shell.execute_reply.started":"2022-03-24T07:08:15.467005Z","shell.execute_reply":"2022-03-24T07:09:10.762317Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"class GRU_LSTM_CNN(nn.Module):\n    def __init__(self, params):\n        super(GRU_LSTM_CNN, self).__init__()\n        self.K_mer_emb_size = params[\"K_mer_emb_size\"]       \n        self.K_mer_nunique = 21             \n        self.antivenom_emb_size = params[\"antivenom_emb_size\"]\n        self.antivenom_unique = 9   \n        self.Kmer_emb_layer = nn.Embedding(\n            num_embeddings=self.K_mer_nunique,\n            embedding_dim=self.K_mer_emb_size,\n        )\n        self.dropout1 = nn.Dropout2d(p=0.2)\n        self.gru = nn.GRU(self.K_mer_emb_size, 64, bidirectional = True, batch_first = True)\n        self.cv1 = nn.Conv1d(128, 32, 3, stride = 2)\n        \n        self.lstm = nn.LSTM(self.K_mer_emb_size, 64, bidirectional= True, batch_first = True)\n        self.cv2 = nn.Conv1d(128, 32, 3, stride = 2)\n        \n        self.fc1 = nn.Sequential(\n                    nn.Linear(4*32, 512),\n                    nn.Dropout(0.2),\n                    nn.ReLU(inplace = True),\n                    nn.Linear(512, 128)\n                    )\n        \n        self.Antivenom_emb = nn.Embedding(\n            num_embeddings=self.antivenom_unique,\n            embedding_dim=self.antivenom_emb_size,\n        )\n    \n        self.protfam_emb = nn.Embedding(\n            num_embeddings=18,\n            embedding_dim=params[\"protfam_emb_size\"],\n        )\n        self.Features = nn.Linear(\n            in_features=self.antivenom_emb_size + params[\"protfam_emb_size\"],\n            out_features=128,\n        )\n        \n        self.fc2 = nn.Sequential(\n                    nn.Linear(256,512),\n                    nn.Dropout(0.2),\n                    nn.ReLU(inplace = True),\n                    nn.Linear(512,256),\n                    nn.Dropout(0.1),\n                    nn.ReLU(inplace = True),\n                    nn.Linear(256,64),\n                    nn.Dropout(0.05),\n                    nn.ReLU(inplace = True),\n                    nn.Linear(64,1))\n\n\n\n    def forward(self,inputs):\n        k_mer = self.Kmer_emb_layer(inputs[\"kmer\"])\n        k_mer = self.dropout1(k_mer)\n        \n        x1, _ = self.gru(k_mer)\n        x1=x1.permute(0, 2, 1)\n        x1 = self.cv1(x1)\n        x1=x1.permute(0, 2, 1)\n        maxpoolx1, _ = torch.max(x1, axis = 1)\n        avgpoolx1 = torch.mean(x1, axis = 1)\n        x1 = torch.cat([maxpoolx1, avgpoolx1], dim = 1)\n        \n        x2, _ = self.lstm(k_mer)\n        x2= x2.permute(0, 2, 1)\n        x2 = self.cv2(x2)\n        x2 = x2.permute(0, 2, 1)\n        maxpoolx2, _ = torch.max(x2, axis = 1)\n        avgpoolx2 = torch.mean(x2, axis = 1)\n        x2 = torch.cat([maxpoolx2, avgpoolx2], dim = 1)\n        \n        x = torch.cat([x1, x2], dim = 1)\n        x = self.fc1(x)\n        \n        \n        antivenom_emb = self.Antivenom_emb(inputs[\"antivenom\"])\n        protfam_emb = self.protfam_emb(inputs[\"protfam\"])\n\n        emb_features = torch.cat((antivenom_emb, protfam_emb), axis=1)\n        features = self.Features(emb_features)        \n        \n        features = torch.cat([features, x], dim = 1)\n        \n        y = self.fc2(features)\n        return y\n","metadata":{"execution":{"iopub.status.busy":"2022-03-24T08:01:59.867646Z","iopub.execute_input":"2022-03-24T08:01:59.867913Z","iopub.status.idle":"2022-03-24T08:01:59.887248Z","shell.execute_reply.started":"2022-03-24T08:01:59.867882Z","shell.execute_reply":"2022-03-24T08:01:59.886476Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-03-24T08:02:00.060960Z","iopub.execute_input":"2022-03-24T08:02:00.061499Z","iopub.status.idle":"2022-03-24T08:02:00.065714Z","shell.execute_reply.started":"2022-03-24T08:02:00.061457Z","shell.execute_reply":"2022-03-24T08:02:00.064868Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"params = {\"K_mer_emb_size\":512, \"protfam_emb_size\": 64, \"antivenom_emb_size\":64}\nfeatures = [\"kmer\", \"antivenom\", \"protfam\"]\nkfold_run(\n    data,\n    features,\n    GRU_LSTM_CNN,\n    params,\n    nsplits=5,\n    loss_fn=nn.MSELoss(),\n    num_epochs=10,\n    device=torch.device(\"cuda\"),\n    batch_size=512,\n    early_stopping=5)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T08:02:00.303696Z","iopub.execute_input":"2022-03-24T08:02:00.303958Z","iopub.status.idle":"2022-03-24T08:28:07.987116Z","shell.execute_reply.started":"2022-03-24T08:02:00.303929Z","shell.execute_reply":"2022-03-24T08:28:07.985669Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"class TimeDistributed(nn.Module):\n    def __init__(self, module, batch_first=False):\n        super(TimeDistributed, self).__init__()\n        self.module = module\n        self.batch_first = batch_first\n\n    def forward(self, x):\n        if len(x.size()) <= 2:\n            return self.module(x)\n        x_reshape = x.contiguous().view(-1, x.size(-1))\n        y = self.module(x_reshape)\n        if self.batch_first:\n            y = y.contiguous().view(x.size(0), -1, y.size(-1))\n        else:\n            y = y.view(-1, x.size(1), y.size(-1))\n        return y\n    \nclass GRU_LSTM_CNN(nn.Module):\n    def __init__(self, params):\n        super(GRU_LSTM_CNN, self).__init__()\n        self.K_mer_emb_size = params[\"K_mer_emb_size\"]       \n        self.K_mer_nunique = 21             \n        self.antivenom_emb_size = params[\"antivenom_emb_size\"]\n        self.antivenom_unique = 9   \n        self.Kmer_emb_layer = nn.Embedding(\n            num_embeddings=self.K_mer_nunique,\n            embedding_dim=self.K_mer_emb_size,\n        )\n        self.init_batchnorm = TimeDistributed(\n            nn.BatchNorm1d(self.K_mer_emb_size, momentum=0.01), batch_first=True\n        )\n        self.dropout1 = nn.Dropout2d(p=0.2)\n        self.gru = nn.GRU(self.K_mer_emb_size, 64, num_layers = 3, bidirectional = True, batch_first = True)\n        self.batchnorm1 = TimeDistributed(\n            nn.BatchNorm1d(128+512, momentum=0.01), batch_first=True\n        )\n        self.cv1 = nn.Conv1d(128+512, 32, 4, stride = 1)\n        \n        self.lstm = nn.LSTM(self.K_mer_emb_size, 64, num_layers = 3, bidirectional= True, batch_first = True)\n        self.batchnorm2 = TimeDistributed(\n            nn.BatchNorm1d(128+512, momentum=0.01), batch_first=True\n        )\n        self.cv2 = nn.Conv1d(128+512, 32, 4, stride = 1)\n        \n        self.fc1 = nn.Sequential(\n                    nn.Linear(4*32, 512),\n                    nn.Dropout(0.2),\n                    nn.ReLU(inplace = True),\n                    nn.Linear(512, 128)\n                    )\n        \n        self.Antivenom_emb = nn.Embedding(\n            num_embeddings=self.antivenom_unique,\n            embedding_dim=self.antivenom_emb_size,\n        )\n    \n        self.protfam_emb = nn.Embedding(\n            num_embeddings=18,\n            embedding_dim=params[\"protfam_emb_size\"],\n        )\n        self.Features = nn.Sequential(\n            nn.Linear(in_features=self.antivenom_emb_size + params[\"protfam_emb_size\"],out_features=256),\n            nn.Dropout(0.2),\n            nn.ReLU(inplace = True),\n            nn.Linear(256, 128)\n        )\n        \n        self.fc2 = nn.Sequential(\n                    nn.Linear(256,512),\n                    nn.Dropout(0.2),\n                    nn.ReLU(inplace = True),\n                    nn.Linear(512,256),\n                    nn.Dropout(0.15),\n                    nn.ReLU(inplace = True),\n                    nn.Linear(256,64),\n                    nn.Dropout(0.1),\n                    nn.ReLU(inplace = True),\n                    nn.Linear(64,1))\n\n\n\n    def forward(self,inputs):\n        k_mer = self.init_batchnorm(self.Kmer_emb_layer(inputs[\"kmer\"]))\n        k_mer = self.dropout1(k_mer)\n        \n        x1, _ = self.gru(k_mer)\n        x1 = self.batchnorm1(torch.cat([x1,k_mer],dim=2))\n        x1=x1.permute(0, 2, 1)\n        x1 = self.cv1(x1)\n        x1=x1.permute(0, 2, 1)\n        maxpoolx1, _ = torch.max(x1, axis = 1)\n        avgpoolx1 = torch.mean(x1, axis = 1)\n        x1 = torch.cat([maxpoolx1, avgpoolx1], dim = 1)\n        \n        x2, _ = self.lstm(k_mer)\n        x2 = self.batchnorm2(torch.cat([x2,k_mer],dim=2))\n        x2= x2.permute(0, 2, 1)\n        x2 = self.cv2(x2)\n        x2 = x2.permute(0, 2, 1)\n        maxpoolx2, _ = torch.max(x2, axis = 1)\n        avgpoolx2 = torch.mean(x2, axis = 1)\n        x2 = torch.cat([maxpoolx2, avgpoolx2], dim = 1)\n        \n        x = torch.cat([x1, x2], dim = 1)\n        x = self.fc1(x)\n        \n        \n        antivenom_emb = self.Antivenom_emb(inputs[\"antivenom\"])\n        protfam_emb = self.protfam_emb(inputs[\"protfam\"])\n\n        emb_features = torch.cat((antivenom_emb, protfam_emb), axis=1)\n        features = self.Features(emb_features)        \n        \n        features = torch.cat([features, x], dim = 1)\n        \n        y = self.fc2(features)\n        return y\n","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:03:16.743313Z","iopub.execute_input":"2022-03-24T13:03:16.743565Z","iopub.status.idle":"2022-03-24T13:03:16.769216Z","shell.execute_reply.started":"2022-03-24T13:03:16.743534Z","shell.execute_reply":"2022-03-24T13:03:16.768488Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"params = {\"K_mer_emb_size\":512, \"protfam_emb_size\": 64, \"antivenom_emb_size\":64}\nfeatures = [\"kmer\", \"antivenom\", \"protfam\"]\nkfold_run(\n    data,\n    features,\n    GRU_LSTM_CNN,\n    params,\n    lr = 1e-3,\n    nsplits=5,\n    loss_fn=nn.MSELoss(),\n    num_epochs=8,\n    device=torch.device(\"cuda\"),\n    batch_size=512,\n    early_stopping=2)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T12:02:03.688807Z","iopub.execute_input":"2022-03-24T12:02:03.689350Z","iopub.status.idle":"2022-03-24T12:24:15.837512Z","shell.execute_reply.started":"2022-03-24T12:02:03.689311Z","shell.execute_reply":"2022-03-24T12:24:15.836751Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"paths = [\"model_fold\"+str(i)+\".pth\" for i in range(1,6)]\ntest_data = Dataset(data, features, state = \"Test\")\ntest_data_loader = DataLoader(test_data, shuffle=False, batch_size=512, num_workers=0)\nsimplesub = predict_test(test_data_loader,paths, torch.device(\"cuda\"))","metadata":{"execution":{"iopub.status.busy":"2022-03-24T12:26:09.014234Z","iopub.execute_input":"2022-03-24T12:26:09.014497Z","iopub.status.idle":"2022-03-24T12:26:48.720476Z","shell.execute_reply.started":"2022-03-24T12:26:09.014467Z","shell.execute_reply":"2022-03-24T12:26:48.719752Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame(columns = [\"ID\", \"Signal\"])\nsubmission[\"ID\"] = data.test[\"ID\"]\nsubmission[\"Signal\"] = 0.2*(simplesub['0']+simplesub['1']+simplesub['2']+simplesub['3']+simplesub['4'])\nplt.hist(submission[\"Signal\"])\nsubmission.to_csv(\"firstmodelubmission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T12:27:00.579561Z","iopub.execute_input":"2022-03-24T12:27:00.580153Z","iopub.status.idle":"2022-03-24T12:27:00.910748Z","shell.execute_reply.started":"2022-03-24T12:27:00.580112Z","shell.execute_reply":"2022-03-24T12:27:00.910068Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"params = {\"K_mer_emb_size\":512, \"protfam_emb_size\": 64, \"antivenom_emb_size\":64}\nfeatures = [\"kmer\", \"antivenom\", \"protfam\"]\nkfold_run(\n    data,\n    features,\n    GRU_LSTM_CNN,\n    params,\n    lr = 1e-3,\n    nsplits=5,\n    loss_fn=nn.MSELoss(),\n    num_epochs=8,\n    device=torch.device(\"cuda\"),\n    batch_size=512,\n    early_stopping=2)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:03:20.571174Z","iopub.execute_input":"2022-03-24T13:03:20.571423Z","iopub.status.idle":"2022-03-24T13:26:09.136019Z","shell.execute_reply.started":"2022-03-24T13:03:20.571393Z","shell.execute_reply":"2022-03-24T13:26:09.135249Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"paths = [\"model_fold\"+str(i)+\".pth\" for i in range(1,6)]\ntest_data = Dataset(data, features, state = \"Test\")\ntest_data_loader = DataLoader(test_data, shuffle=False, batch_size=512, num_workers=0)\nsimplesub = predict_test(test_data_loader,paths, torch.device(\"cuda\"))","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:26:19.777534Z","iopub.execute_input":"2022-03-24T13:26:19.778247Z","iopub.status.idle":"2022-03-24T13:26:58.794179Z","shell.execute_reply.started":"2022-03-24T13:26:19.778206Z","shell.execute_reply":"2022-03-24T13:26:58.793333Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame(columns = [\"ID\", \"Signal\"])\nsubmission[\"ID\"] = data.test[\"ID\"]\nsubmission[\"Signal\"] = 0.2*(simplesub['0']+simplesub['1']+simplesub['2']+simplesub['3']+simplesub['4'])\nplt.hist(submission[\"Signal\"])\nsubmission.to_csv(\"firstmodelubmission.csv\", index = False)","metadata":{"execution":{"iopub.status.busy":"2022-03-24T13:27:06.279078Z","iopub.execute_input":"2022-03-24T13:27:06.279369Z","iopub.status.idle":"2022-03-24T13:27:06.595110Z","shell.execute_reply.started":"2022-03-24T13:27:06.279336Z","shell.execute_reply":"2022-03-24T13:27:06.594455Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}